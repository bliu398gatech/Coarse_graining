{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glCGch4G8BqO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.stats import pearsonr\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "# Function to generate an Ornstein-Uhlenbeck (OU) process\n",
        "def generate_OU_process(tau, dt, T):\n",
        "    \"\"\"\n",
        "    Generate an Ornstein-Uhlenbeck process with zero mean and unit variance.\n",
        "\n",
        "    Parameters:\n",
        "    - tau: Time constant of the OU process.\n",
        "    - dt: Time step size.\n",
        "    - T: Total time duration.\n",
        "\n",
        "    Returns:\n",
        "    - y: Array containing the OU process values.\n",
        "    \"\"\"\n",
        "    N = int(T / dt)\n",
        "    y = np.zeros(N)\n",
        "    for i_t in range(1, N):\n",
        "        y[i_t] = (1 - dt / tau) * y[i_t - 1] + np.sqrt(2 * dt / tau) * np.random.randn()\n",
        "    return y\n",
        "\n",
        "# Function to merge clusters based on activity correlations\n",
        "def merge_clusters(clusters, data):\n",
        "    \"\"\"\n",
        "    Merge pairs of clusters in the cluster list to form new clusters of double the size.\n",
        "\n",
        "    Parameters:\n",
        "    - clusters: List of clusters; clusters[i] contains neuron indices in cluster i.\n",
        "    - data: Original data matrix used to compute correlations.\n",
        "\n",
        "    Returns:\n",
        "    - new_clusters: List of merged clusters.\n",
        "    \"\"\"\n",
        "    num_clusters = len(clusters)\n",
        "    new_clusters = []\n",
        "    merged_flags = [False] * num_clusters  # Flags to mark merged clusters\n",
        "\n",
        "    # Compute the activity (sum of neuron activities within the cluster) for each cluster\n",
        "    cluster_activities = []\n",
        "    for cluster_indices in clusters:\n",
        "        cluster_data = data[:, cluster_indices]\n",
        "        # Sum activities of neurons\n",
        "        activity = np.sum(cluster_data, axis=1)\n",
        "        # Normalize to have a mean of 1 for non-zero entries\n",
        "        nonzero_indices = activity != 0\n",
        "        if np.any(nonzero_indices):\n",
        "            mean_nonzero = np.mean(activity[nonzero_indices])\n",
        "            activity[nonzero_indices] = activity[nonzero_indices] / mean_nonzero\n",
        "        cluster_activities.append(activity)\n",
        "\n",
        "    # Compute correlation matrix between clusters\n",
        "    num_activities = len(cluster_activities)\n",
        "    corr_matrix = np.full((num_activities, num_activities), -np.inf)  # Initialize to -inf to avoid self-correlation\n",
        "\n",
        "    for i in range(num_activities):\n",
        "        for j in range(i + 1, num_activities):\n",
        "            corr_value, _ = pearsonr(cluster_activities[i], cluster_activities[j])\n",
        "            corr_matrix[i, j] = corr_value\n",
        "\n",
        "    # Begin merging process\n",
        "    while np.sum(~np.array(merged_flags)) >= 2:\n",
        "        # Find the pair of clusters with the highest correlation among unmerged clusters\n",
        "        available_indices = [idx for idx, flag in enumerate(merged_flags) if not flag]\n",
        "        sub_corr_matrix = corr_matrix[np.ix_(available_indices, available_indices)]\n",
        "\n",
        "        # Find the maximum correlation and its indices\n",
        "        idx = np.argmax(sub_corr_matrix)\n",
        "        row, col = np.unravel_index(idx, sub_corr_matrix.shape)\n",
        "        i = available_indices[row]\n",
        "        j = available_indices[col]\n",
        "\n",
        "        # Merge clusters i and j\n",
        "        merged_cluster = clusters[i] + clusters[j]\n",
        "        new_clusters.append(merged_cluster)\n",
        "\n",
        "        # Mark clusters as merged\n",
        "        merged_flags[i] = True\n",
        "        merged_flags[j] = True\n",
        "\n",
        "    # If there are remaining unmerged clusters, discard them\n",
        "    if any(not flag for flag in merged_flags):\n",
        "        num_discarded = sum(not flag for flag in merged_flags)\n",
        "        print(f'Discarded {num_discarded} unmerged clusters to meet cluster size requirement.')\n",
        "\n",
        "    return new_clusters\n",
        "\n",
        "# Function to compute average eigenvalues and total variance for clusters\n",
        "def compute_average_eigenvalues(original_data, clusters, K):\n",
        "    \"\"\"\n",
        "    Compute the average eigenvalue spectrum and total variance for given clusters.\n",
        "\n",
        "    Parameters:\n",
        "    - original_data: Original data matrix [num_time_steps x num_neurons_initial].\n",
        "    - clusters: List of clusters; clusters[i] contains neuron indices in cluster i.\n",
        "    - K: Current cluster size.\n",
        "\n",
        "    Returns:\n",
        "    - mean_eigenvalues: Mean eigenvalues [1 x K].\n",
        "    - rank_K: Rank/K values [1 x K].\n",
        "    - total_variances: Total variance for each cluster [num_clusters x 1].\n",
        "    \"\"\"\n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    # Store eigenvalues for each cluster\n",
        "    eigenvalues_all_clusters = np.zeros((num_clusters, K))\n",
        "    total_variances = np.zeros(num_clusters)\n",
        "\n",
        "    for c in range(num_clusters):\n",
        "        cluster_indices = clusters[c]  # Indices of neurons in the current cluster\n",
        "\n",
        "        # Check if cluster size equals K\n",
        "        if len(cluster_indices) != K:\n",
        "            raise ValueError(f'Cluster size not equal to K = {K}, actual size is {len(cluster_indices)}.')\n",
        "\n",
        "        # Get spike data for the cluster\n",
        "        cluster_spike_data = original_data[:, cluster_indices]  # [num_time_steps x K]\n",
        "\n",
        "        # Remove mean\n",
        "        cluster_spike_data = cluster_spike_data - np.mean(cluster_spike_data, axis=0)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov_matrix = np.cov(cluster_spike_data, rowvar=False)  # [K x K]\n",
        "\n",
        "        # Compute eigenvalues and sort in descending order\n",
        "        eigenvalues = np.linalg.eigvalsh(cov_matrix)\n",
        "        eigenvalues_sorted = np.sort(eigenvalues)[::-1]\n",
        "\n",
        "        # Store eigenvalues\n",
        "        eigenvalues_all_clusters[c, :] = eigenvalues_sorted\n",
        "\n",
        "        # Compute total variance (sum of eigenvalues)\n",
        "        total_variances[c] = np.sum(eigenvalues_sorted)\n",
        "\n",
        "    # For each rank, compute the mean eigenvalue across all clusters\n",
        "    mean_eigenvalues = np.mean(eigenvalues_all_clusters, axis=0)\n",
        "    rank_K = np.arange(1, K + 1) / K\n",
        "    return mean_eigenvalues, rank_K, total_variances\n",
        "\n",
        "# Main program\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Define ranges for eta and epsilon\n",
        "    eta_vals = np.arange(5, 6 + 0.5, 0.5)  # eta from 5 to 6 with step size 0.5\n",
        "    epsilon_vals = np.arange(-18, 18 + 1, 1)  # epsilon from -18 to 18 with step size 1\n",
        "    num_eta = len(eta_vals)\n",
        "    num_eps = len(epsilon_vals)\n",
        "\n",
        "    # Set Nf\n",
        "    Nf = 5\n",
        "\n",
        "    # Initialize matrices to store R^2 values and slope z\n",
        "    R2_values = np.full((num_eta, num_eps), np.nan)  # R^2 values for each (eta, epsilon)\n",
        "    z_values = np.full((num_eta, num_eps), np.nan)   # Slope z for each (eta, epsilon)\n",
        "\n",
        "    # Pre-generate h_m_finitetau and norm_J_vals\n",
        "    bin_size = 2\n",
        "    max_dim = Nf\n",
        "    N_T = 100000  # Number of time steps\n",
        "    num_neur = 512\n",
        "    bin_size_xc = bin_size\n",
        "    bin_size0 = 2\n",
        "    finite_tau = 1000  # Time constant\n",
        "    dt = 1  # Time step size\n",
        "\n",
        "    # Generate h_m_finitetau\n",
        "    h_m_finitetau = np.zeros((int(N_T * (bin_size_xc / bin_size0)), max_dim))\n",
        "    for i_d in range(max_dim):\n",
        "        h_m_finitetau[:, i_d] = generate_OU_process(finite_tau, dt, N_T * (bin_size_xc / bin_size0))\n",
        "\n",
        "    # Generate random weights norm_J_vals\n",
        "    norm_J_vals = np.random.randn(num_neur, max_dim).T  # Transpose to match dimensions\n",
        "\n",
        "    # Create folder to save images (absolute path)\n",
        "    output_folder = os.path.join(os.getcwd(), 'Eigenvalue_Plots_Nf_5')\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Safe to call in parallel environment\n",
        "\n",
        "    # Define function to process each eta value\n",
        "    def process_eta(i_eta):\n",
        "        this_eta_val = eta_vals[i_eta]\n",
        "\n",
        "        # Initialize arrays to store results for current eta\n",
        "        R2_values_i_eta = np.full(num_eps, np.nan)\n",
        "        z_values_i_eta = np.full(num_eps, np.nan)\n",
        "\n",
        "        for i_eps in range(num_eps):\n",
        "            this_eps_val = epsilon_vals[i_eps]\n",
        "            print(f'Processing eta = {this_eta_val:.2f}, epsilon = {this_eps_val:.2f}')\n",
        "\n",
        "            # Simulate data using the same J and h\n",
        "            def p_s_i_fun(h_m, J_m, eta_val, epsilon_val):\n",
        "                return 1.0 / (1 + np.exp(-eta_val * (h_m @ J_m) - epsilon_val))\n",
        "\n",
        "            # Compute p_s_i_vals_ft using pre-generated h_m_finitetau and norm_J_vals\n",
        "            p_s_i_vals_ft = p_s_i_fun(h_m_finitetau[:, :max_dim], norm_J_vals, this_eta_val, this_eps_val)\n",
        "\n",
        "            # Ensure p_s_i_vals_ft is within [0, 1]\n",
        "            p_s_i_vals_ft = np.clip(p_s_i_vals_ft, np.finfo(float).eps, 1 - np.finfo(float).eps)\n",
        "\n",
        "            # Generate spike counts\n",
        "            sc_i_vals_ft = np.random.rand(*p_s_i_vals_ft.shape) < p_s_i_vals_ft\n",
        "            binned_sc_i_ft = np.sum(sc_i_vals_ft.reshape(int(bin_size_xc / bin_size0), N_T, num_neur), axis=0)\n",
        "\n",
        "            # Remove neurons with low activity\n",
        "            nonsilent_cells = np.sum(binned_sc_i_ft, axis=0) > 5\n",
        "            binned_sc_i_ft = binned_sc_i_ft[:, nonsilent_cells]\n",
        "\n",
        "            data = binned_sc_i_ft  # [num_time_steps x num_neurons_initial]\n",
        "            num_time_steps, num_neurons_initial = data.shape\n",
        "            print(f'The size of spike count matrix is {num_time_steps} x {num_neurons_initial}.')\n",
        "\n",
        "            # Begin coarse-graining code\n",
        "            # Set K values\n",
        "            target_K_values = [2, 4, 8, 16, 32, 64, 128]\n",
        "            num_K = len(target_K_values)\n",
        "\n",
        "            # Initialize variables\n",
        "            clusters_rounds = [None] * num_K       # Clusters for each round\n",
        "            mean_eigenvalues_by_K = [None] * num_K  # Mean eigenvalues for each round\n",
        "            rank_K = [None] * num_K                # Rank/K for each round\n",
        "            slopes_z = np.full(num_K, np.nan)      # Slope z for each K\n",
        "            mean_variances_by_K = np.full(num_K, np.nan)  # Mean variance for each round\n",
        "\n",
        "            # Initialize clustering\n",
        "            initial_clusters = [[i] for i in range(num_neurons_initial)]  # clusters[i] contains neuron indices\n",
        "            current_clusters = initial_clusters.copy()\n",
        "\n",
        "            # Begin coarse-graining rounds\n",
        "            for round_idx in range(num_K):\n",
        "                K = target_K_values[round_idx]\n",
        "                print(f'\\n===== Round {round_idx + 1}: K = {K} =====')\n",
        "\n",
        "                # Calculate the number of clusters needed and the number of merges\n",
        "                previous_cluster_size = K // 2  # Previous cluster size\n",
        "                num_previous_clusters = len(current_clusters)\n",
        "\n",
        "                # Check if there are enough clusters to merge\n",
        "                if previous_cluster_size < 1 or num_previous_clusters % 2 != 0:\n",
        "                    # If the number of clusters is not even, discard extra clusters\n",
        "                    num_clusters_to_use = (num_previous_clusters // 2) * 2\n",
        "                    print(f'Discarded {num_previous_clusters - num_clusters_to_use} clusters to meet cluster size requirement.')\n",
        "                    current_clusters = current_clusters[:num_clusters_to_use]\n",
        "\n",
        "                # Perform merging\n",
        "                new_clusters = merge_clusters(current_clusters, data)\n",
        "\n",
        "                # Store clusters for this round\n",
        "                clusters_rounds[round_idx] = new_clusters\n",
        "\n",
        "                # Compute average eigenvalues and total variance for this round\n",
        "                mean_eigenvalues, rank_K_round, total_variances = compute_average_eigenvalues(data, new_clusters, K)\n",
        "                mean_eigenvalues_by_K[round_idx] = mean_eigenvalues\n",
        "                rank_K[round_idx] = rank_K_round\n",
        "\n",
        "                # Compute mean variance\n",
        "                mean_variance = np.mean(total_variances)\n",
        "                mean_variances_by_K[round_idx] = mean_variance\n",
        "\n",
        "                print(f'Round {round_idx + 1} completed: formed {len(new_clusters)} clusters, average eigenvalues and variance calculation completed.')\n",
        "\n",
        "                # Prepare for next round\n",
        "                current_clusters = new_clusters.copy()\n",
        "\n",
        "            # Plot and save eigenvalue images, compute R^2 and z\n",
        "            # Fit for K >= 32\n",
        "            idx_K_list = [idx for idx, K_val in enumerate(target_K_values) if K_val >= 32]  # Indices where K >= 32\n",
        "            combined_log_rank_K = []\n",
        "            combined_log_eigenvalues = []\n",
        "\n",
        "            # Create a figure for current eta and epsilon\n",
        "            fig, ax = plt.subplots()\n",
        "\n",
        "            for idx in idx_K_list:\n",
        "                K_i = target_K_values[idx]\n",
        "                if mean_eigenvalues_by_K[idx] is None:\n",
        "                    continue  # Skip if no data\n",
        "                rank_K_round = rank_K[idx]\n",
        "                mean_eigenvalue = mean_eigenvalues_by_K[idx]\n",
        "\n",
        "                # Remove NaN values\n",
        "                valid_indices = ~np.isnan(mean_eigenvalue)\n",
        "                rank_K_round = rank_K_round[valid_indices]\n",
        "                mean_eigenvalue = mean_eigenvalue[valid_indices]\n",
        "\n",
        "                # Ensure they are arrays\n",
        "                rank_K_round = rank_K_round.flatten()\n",
        "                mean_eigenvalue = mean_eigenvalue.flatten()\n",
        "\n",
        "                # Logarithmic values\n",
        "                log_rank_K = np.log(rank_K_round)\n",
        "                log_eigenvalues = np.log(mean_eigenvalue)\n",
        "\n",
        "                # Determine fitting range (Rank/K >= 0 & Rank/K <= 0.3)\n",
        "                fit_range = (rank_K_round >= 0) & (rank_K_round <= 0.3)\n",
        "\n",
        "                # Check if there are enough data points\n",
        "                if np.sum(fit_range) < 2:\n",
        "                    print(f'Warning: K = {K_i} does not have enough data points for fitting.')\n",
        "                    slopes_z[idx] = np.nan\n",
        "                    continue\n",
        "\n",
        "                # Linear fit\n",
        "                p = np.polyfit(log_rank_K[fit_range], log_eigenvalues[fit_range], 1)\n",
        "\n",
        "                # Record slope z\n",
        "                slopes_z[idx] = p[0]\n",
        "\n",
        "                # Plot curve\n",
        "                ax.plot(rank_K_round, mean_eigenvalue, 'o-', label=f'K = {K_i}')\n",
        "\n",
        "                # Plot fitted line\n",
        "                fitted_line = np.exp(np.polyval(p, log_rank_K[fit_range]))\n",
        "                ax.plot(rank_K_round[fit_range], fitted_line, '--', linewidth=1)\n",
        "\n",
        "                # Collect data for combined fit\n",
        "                combined_log_rank_K.extend(log_rank_K[fit_range])\n",
        "                combined_log_eigenvalues.extend(log_eigenvalues[fit_range])\n",
        "\n",
        "            # Perform combined fit over all K values\n",
        "            if len(combined_log_rank_K) >= 2:\n",
        "                # Linear fit\n",
        "                p_combined = np.polyfit(combined_log_rank_K, combined_log_eigenvalues, 1)\n",
        "                z_combined = p_combined[0]\n",
        "                log_eigenvalues_fit_combined = np.polyval(p_combined, combined_log_rank_K)\n",
        "\n",
        "                # Compute combined R^2\n",
        "                residuals_combined = np.array(combined_log_eigenvalues) - log_eigenvalues_fit_combined\n",
        "                SS_res_combined = np.sum(residuals_combined ** 2)\n",
        "                SS_tot_combined = np.sum((np.array(combined_log_eigenvalues) - np.mean(combined_log_eigenvalues)) ** 2)\n",
        "                R2_combined = 1 - SS_res_combined / SS_tot_combined\n",
        "\n",
        "                # Save R2 and z\n",
        "                R2_values_i_eta[i_eps] = R2_combined\n",
        "                z_values_i_eta[i_eps] = z_combined\n",
        "            else:\n",
        "                R2_values_i_eta[i_eps] = np.nan\n",
        "                z_values_i_eta[i_eps] = np.nan\n",
        "\n",
        "            # Finalize plot\n",
        "            ax.set_xlabel('Rank / K')\n",
        "            ax.set_ylabel('Mean Eigenvalue')\n",
        "            ax.set_title(f'Mean Eigenvalue vs Rank / K (eta={this_eta_val:.2f}, epsilon={this_eps_val:.2f})')\n",
        "            ax.set_xscale('log')\n",
        "            ax.set_yscale('log')\n",
        "            ax.legend(loc='best')\n",
        "            ax.grid(True)\n",
        "\n",
        "            # Save image\n",
        "            filename = f'Eigenvalue_vs_RankK_eta{this_eta_val:.2f}_eps{this_eps_val:.2f}.png'\n",
        "            filename = filename.replace('.', '_').replace('-', 'neg')\n",
        "            plt.savefig(os.path.join(output_folder, filename))\n",
        "            plt.close(fig)\n",
        "\n",
        "            print(f'Eta = {this_eta_val:.2f}, Epsilon = {this_eps_val:.2f}, Combined R^2 = {R2_values_i_eta[i_eps]:.4f}, z = {z_values_i_eta[i_eps]:.4f}')\n",
        "\n",
        "        # Return results for current eta\n",
        "        return R2_values_i_eta, z_values_i_eta\n",
        "\n",
        "    # Begin parallel processing over eta values\n",
        "    if __name__ == '__main__':\n",
        "        # Use multiprocessing pool for parallel computation\n",
        "        num_processes = min(6, os.cpu_count())  # Adjust number of processes based on system\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            # Map the process_eta function to each i_eta\n",
        "            results = pool.map(process_eta, range(num_eta))\n",
        "\n",
        "        # Collect results\n",
        "        for i_eta, (R2_values_i_eta, z_values_i_eta) in enumerate(results):\n",
        "            R2_values[i_eta, :] = R2_values_i_eta\n",
        "            z_values[i_eta, :] = z_values_i_eta\n",
        "\n",
        "        # Save the R2_values and z_values arrays\n",
        "        np.save(os.path.join(output_folder, 'R2_values.npy'), R2_values)\n",
        "        np.save(os.path.join(output_folder, 'z_values.npy'), z_values)\n",
        "\n",
        "        print(f'Total computation time: {time.time() - start_time:.2f} seconds')\n"
      ]
    }
  ]
}